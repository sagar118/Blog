{
  
    
        "post0": {
            "title": "Song Popularity EDA",
            "content": "This Python notebook is the Python version of Song Popularity EDA - Live Coding Fun by Martin Henze . Purpose of this notebook is to recreate the plots in python for learning purpose. . The recording of the live-coding session can be found on Abhishek Thakur&#39;s YouTube channel: . Song Popularity Prediction - EDA (Part 1) | Song Popularity Prediction - EDA (Part 2) | . 1. Introduction . The competition is about Song Prediction based on a set of different features. The dataset contains the basic file such as train.csv, test.csv and submission_sample.csv. The dataset used in this competition is in tabular format. The evaluation metric used for this competition is AUC score. . 2. Preparation . Initially we&#39;ll load different libraries used in our analysis. Also, load the train and test data. . &quot;&quot;&quot;Load the libraries&quot;&quot;&quot; import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import os import random import warnings from plotnine import * %matplotlib inline warnings.filterwarnings(&quot;ignore&quot;) . . &quot;&quot;&quot;Load the data&quot;&quot;&quot; train = pd.read_csv(&quot;/kaggle/input/song-popularity-prediction/train.csv&quot;) test = pd.read_csv(&quot;/kaggle/input/song-popularity-prediction/test.csv&quot;) . . 3. Overview: structure and data content . The first step we&#39;ll do is look at the raw data. This tell us about the different features in the dataset, missing values, and types of features (numeric, string, categorical, etc.). . 3.1. Look at the data . Let&#39;s look at the basic structure of the data . print(&#39; nInformation about Data&#39;) display(train.info()) . . We find: . There are 40000 entries and 15 features in total. | All the column data type is either int or float i.e. all the columns are numeric. This make is comparatively easier to work with compared to columns contains string type data. | We can also observe there are columns that contain less than 40K Non-Null values which indicates missing values in the dataset. | . Let&#39;s now look at the top 20 rows of the data. . &quot;&quot;&quot;Display top 20 rows of the train data&quot;&quot;&quot; display(train.head(20).style.set_caption(&quot;First Twenty rows of Training Data&quot;)) . . We find: . There are missing values that can be seen as nan in the table above | The id column seems to have values in increasing order | The values in the features are in different scales | . Now, let&#39;s look at some basic statistics about our features in the data . display(train.describe().style.set_caption(&quot;Basic statistics about Train Data&quot;)) . . We find: . Most of the features are in the range of 0 and 1 | There are features with only negative values (loudness), binary features (audio_mode) , and seems to be categorical (key and time_signature) | . 3.2. Missing data . Now let&#39;s take a closer look at the missing values in the dataset . &quot;&quot;&quot;Missing Values&quot;&quot;&quot; print(f&quot;Train set has {train.isnull().sum().sum()} missing values, and test set has {test.isnull().sum().sum()} missing values&quot;) . . # Refrence (edited): https://datavizpyr.com/visualizing-missing-data-with-seaborn-heatmap-and-displot/ fig = plt.figure(figsize=(18,6)) sns.displot( data=train.isna().melt(value_name=&quot;missing&quot;), y=&quot;variable&quot;, hue=&quot;missing&quot;, multiple=&quot;fill&quot;, aspect=3 ) plt.title(&quot;Missing values shown using Bar plot&quot;, fontsize=17) plt.ylabel(&quot;&quot;) plt.xlabel(&quot;&quot;) plt.figure(figsize=(18,10)) sns.heatmap(train.isna().transpose()) plt.title(&#39;Heatmap showing Missing Values in Train data&#39;, fontsize=17) plt.ylabel(&quot;&quot;) plt.xlabel(&quot;&quot;) plt.show() . . train_null = train.isna().sum().sort_values(ascending = False) test_null = test.isna().sum().sort_values(ascending = False) non_zero_train_values = train_null[train_null.values &gt; 0] non_zero_test_values = test_null[test_null.values &gt; 0] fig, axes = plt.subplots(1,2, figsize=(15,8)) sns.barplot(y=non_zero_test_values.index , x=non_zero_test_values.values, ax=axes[1], palette = &quot;viridis&quot;) sns.barplot(y=non_zero_train_values.index , x=non_zero_train_values.values, ax=axes[0], palette = &quot;viridis&quot;) axes[0].set_title(&quot;Train data&quot;, fontsize=14) axes[1].set_title(&quot;Test data&quot;, fontsize=14) plt.tight_layout() plt.show() . . 4. Visualization - Individual Features . After getting an initial idea about our features and their values, we can now dive into the visual part of the exploration. I recommend to always plot your data. Sometimes this might be challenging, e.g. because you have tons of features. In that case, you want to start at least with a subset before you run any dimensionality reduction or other tools. This step is as much about spotting issues and irregularities as it is about learning more about the shapes and distributions of your features. . 4.1. Predictor variables . In the live session, we were building this plot step by step. (Well, we got most of the way there.) It really pays off to take the time and investigate each feature separately. This is one of the most instructive steps in the EDA process, where you aim to learn how messed up your features are. No dataset is perfect. We want to figure out how severe those imperfections are, and whether we can live with them or have to address them. | Different kind of data types go best with different kind of visuals. My recommendation is to start out with density plots or histograms for numerical features, and with barcharts for those that are better expressed as types of categories. | . useful_cols = [col for col in train.columns if col not in [&quot;id&quot;, &quot;song_popularity&quot;]] numeric_cols = [col for col in useful_cols if col not in [&quot;key&quot;, &quot;audio_mode&quot;, &quot;time_signature&quot;]] n_rows = 5 n_cols = 3 index = 1 colors = [&quot;red&quot;, &quot;darkblue&quot;, &quot;green&quot;] fig = plt.figure(figsize=(16,20)) for index, col in enumerate(train[useful_cols].columns): plt.subplot(n_rows,n_cols,index+1) if col in numeric_cols: sns.kdeplot(train[col], color=random.sample(colors, 1), fill=True) plt.title(col, fontsize=14) plt.xlabel(&quot;&quot;) plt.ylabel(&quot;&quot;) plt.tight_layout() else: sns.countplot(train[col]) plt.title(col, fontsize=14) plt.xlabel(&quot;&quot;) plt.ylabel(&quot;&quot;) plt.tight_layout() plt.subplot(n_rows,n_cols,14) sns.kdeplot(np.log(train[&#39;instrumentalness&#39;]), color=random.sample(colors, 1), fill=True) plt.title(&#39;instrumentalness (log transformed)&#39;, fontsize=14) plt.ylabel(&quot; &quot;) plt.xlabel(&quot; &quot;) plt.tight_layout() plt.show() . . We find: . Our initial impressions of the data types have largely been confirmed: audio_mode is a boolean feature, and time_signature and key are ordinal or categorical ones (or integer; although a better understanding of those musical concepts would certainly benefit from some domain knowledge.) | A number of features are bounded between 0 and 1: accosticness, danceability, energy, liveliness, speechiness, and audio_valence. | The feature loudness looks like it refer to the decibel scale. | The distribution of instrumentalness is heavily right-skewed, and even after a log transform this feature doesn’t look very well-behaved. This might need a bit more work. | . 4.2. Target: Song Popularity . On to the target itself. We figured out that song_popularity is a binary feature, and thus we can express it as boolean. Here we plot a barchart. . sns.countplot(train.song_popularity.astype(&quot;bool&quot;)) plt.title(&quot;Target: Song Popularity&quot;, fontsize=14) plt.xlabel(&quot;&quot;) plt.ylabel(&quot;&quot;) plt.show() . . We find: . There is a slight imbalance in the target distribution: a bit more than 60/40. Not super imbalanced, but something to keep in mind. | . 5. Feature interactions . After learning more about each individual feature, we now want to see them interacting with one another. It’s best to perfom those steps in that order, so that you can understand and interpret the interactions in the context of the overall distributions. . 5.1. Target impact . We have seen all the feature distributions, now we want to investigate whether they look different based on the target value. Here’s an example for song_duration: . fig = plt.figure(figsize=(16,18)) n_rows = 4 n_cols = 3 for index, col in enumerate(numeric_cols): plt.subplot(n_rows, n_cols, index+1) sns.kdeplot(train[col], hue=train.song_popularity.astype(&quot;bool&quot;), fill=True) plt.title(col, fontsize=14) plt.xlabel(&quot;&quot;) plt.ylabel(&quot;&quot;) plt.tight_layout() plt.show() . . Observations: . By looking at the probability distribution of different variables we find that popular songs are almost exactly the same length as unpopular ones. There is a slight difference, but it’s pretty small. | . Now we can check the categorical features. . fig = plt.figure(figsize=(18,5)) for index, col in enumerate([&quot;key&quot;, &quot;audio_mode&quot;, &quot;time_signature&quot;]): plt.subplot(1,3,index+1) sns.countplot(train[col], hue=train.song_popularity.astype(&quot;bool&quot;)) plt.title(col, fontsize=14) plt.xlabel(&quot;&quot;) plt.ylabel(&quot;&quot;) plt.tight_layout() plt.show() . . 5.2. Feature Interaction . How do the predictor features interact with each other? Are there any redundancies or strong relationships? We will start out with a correlation matrix, and then look at features of interest in a bit more detail. . 5.2.1. Correlations overview . # Refrence (edited): https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec def heatmap(data): corr = pd.melt(data.reset_index(), id_vars=&#39;index&#39;) # Unpivot the dataframe, so we can get pair of arrays for x and y corr.columns = [&#39;x&#39;, &#39;y&#39;, &#39;value&#39;] x=corr[&#39;x&#39;] y=corr[&#39;y&#39;] size=corr[&#39;value&#39;].abs() color=corr[&#39;value&#39;] fig, ax = plt.subplots(figsize=(10,10)) plot_grid = plt.GridSpec(1, 15, hspace=0.2, wspace=0.1) # Setup a 1x15 grid ax = plt.subplot(plot_grid[:,:-1]) # Use the leftmost 14 columns of the grid for the main plot n_colors = 256 # Use 256 colors for the diverging color palette palette = sns.diverging_palette(20, 220, n=n_colors) # Create the palette color_min, color_max = [-1, 1] # Range of values that will be mapped to the palette, i.e. min and max possible correlation size_min, size_max = 0, 1 def value_to_color(val): val_position = float((val - color_min)) / (color_max - color_min) # position of value in the input range, relative to the length of the input range val_position = min(max(val_position, 0), 1) # bound the position betwen 0 and 1 ind = int(val_position * (n_colors - 1)) # target index in the color palette return palette[ind] def value_to_size(val): val_position = (val - size_min) * 0.99 / (size_max - size_min) + 0.01 # position of value in the input range, relative to the length of the input range val_position = min(max(val_position, 0), 1) # bound the position betwen 0 and 1 return val_position * size_scale # Mapping from column names to integer coordinates x_labels = [v for v in sorted(x.unique())] y_labels = [v for v in sorted(y.unique())] x_to_num = {p[1]:p[0] for p in enumerate(x_labels)} y_to_num = {p[1]:p[0] for p in enumerate(y_labels)} size_scale = 500 ax.scatter( x=x.map(x_to_num), # Use mapping for x y=y.map(y_to_num), # Use mapping for y s=size.apply(value_to_size), # Vector of square sizes, proportional to size parameter c=color.apply(value_to_color), # Vector of square color values, mapped to color palette marker=&#39;s&#39; # Use square as scatterplot marker ) # Show column labels on the axes ax.set_xticks([x_to_num[v] for v in x_labels]) ax.set_xticklabels(x_labels, rotation=45, horizontalalignment=&#39;right&#39;) ax.set_yticks([y_to_num[v] for v in y_labels]) ax.set_yticklabels(y_labels) ax.grid(False, &#39;major&#39;) ax.grid(True, &#39;minor&#39;) ax.set_xticks([t + 0.5 for t in ax.get_xticks()], minor=True) ax.set_yticks([t + 0.5 for t in ax.get_yticks()], minor=True) ax.set_xlim([-0.5, max([v for v in x_to_num.values()]) + 0.5]) ax.set_ylim([-0.5, max([v for v in y_to_num.values()]) + 0.5]) ax.set_facecolor(&#39;#F1F1F1&#39;) # Add color legend on the right side of the plot ax = plt.subplot(plot_grid[:,-1]) # Use the rightmost column of the plot col_x = [0]*len(palette) # Fixed x coordinate for the bars bar_y=np.linspace(color_min, color_max, n_colors) # y coordinates for each of the n_colors bars bar_height = bar_y[1] - bar_y[0] ax.barh( y=bar_y, width=[5]*len(palette), # Make bars 5 units wide left=col_x, # Make bars start at 0 height=bar_height, color=palette, linewidth=0 ) ax.set_xlim(1, 2) # Bars are going from 0 to 5, so lets crop the plot somewhere in the middle ax.grid(False) # Hide grid ax.set_facecolor(&#39;white&#39;) # Make background white ax.set_xticks([]) # Remove horizontal ticks ax.set_yticks(np.linspace(min(bar_y), max(bar_y), 3)) # Show vertical ticks for min, middle and max ax.yaxis.tick_right() # Show vertical ticks on the right . . heatmap(train[numeric_cols].corr()) . . Below is a similar correlation heatmap but only using the lower triangle to show the correlation. . # Refrence (edited): https://medium.com/@szabo.bibor/how-to-create-a-seaborn-correlation-heatmap-in-python-834c0686b88e fig = plt.figure(figsize=(10,10)) matrix = np.triu(np.ones_like(train[numeric_cols].corr(), dtype=np.bool)) sns.heatmap(train[numeric_cols].corr(), mask=matrix, vmin=-1, vmax=1, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True)) plt.show() . . We find: . There’s a strong anti-correlation between acousticness vs energy and loudness, respectively. Consequently, energy and loudness share a strong correlation. | None of the features individually show a notable correlation with the target song_popularity. | . 5.2.2. Categorical feature interactions . Whenever we’re looking at categorical features, we can assign a visualisation dimension like colour, size, or facets to those. We will start modifying our trusted density plots to look at the distributions of energy (potentially one of the more interesting numerical features) for the different values of time_signature (here encoded as colour): . fig = plt.figure(figsize=(10,8)) sns.kdeplot(x=&quot;energy&quot;, hue=&quot;time_signature&quot;, data=train, fill=True, bw=0.03) plt.show() . . (ggplot(train, aes(&quot;key&quot;, &quot;time_signature&quot;, fill = &quot;energy&quot;)) + geom_tile() + theme(figure_size=(16,5)) + scale_x_continuous(breaks=range(0,12))) . . We find: . For time_signatures 2 and 5 we have no instances of key == 11. This is no big surprise, since those three values are already rare individually, which makes their combinations even more rare. . | There are no clear clusters of high vs low energy features here. . | We can see certain combinations that are particularly low energy, such as key == 2 and time_signature == 1 or 8. key == 3 and time_signature == 1 seems to be a particularly energetic combination. . | . 5.3. Feature Target Interaction . Once we have found interesting correlations we can look for clustering in the target variable. . (ggplot(train, aes(&#39;key&#39;, &#39;time_signature&#39;)) + geom_tile(aes(fill=&#39;energy&#39;)) + facet_wrap(&quot;song_popularity&quot;, nrow = 2) + theme_minimal() + theme(figure_size=(16, 8)) + scale_x_continuous(breaks=range(0,12))) . . sns.displot(data=train, x=&quot;energy&quot;, y=&quot;audio_valence&quot;, col=&quot;song_popularity&quot;, kind=&quot;kde&quot;, fill=True, legend=True, height=8, aspect=0.75) plt.show() . . fig = plt.figure(figsize=(12,6)) sns.scatterplot(x=&quot;energy&quot;, y=&quot;acousticness&quot;, hue=&quot;song_popularity&quot;, data=train) plt.show() . . More Resources: . Choosing different color palette in Seaborn | See also PairGrid: Subplot grid for plotting pairwise relationships | relplot: Combine a relational plot and a FacetGrid | displot: Combine a distribution plot and a FacetGrid | catplot: Combine a categorical plot and a FacetGrid | lmplot: Combine a regression plot and a FacetGrid | . | . Special Thanks to Martin Henze for sharing his knowledge during the live coding session. Also, thank you Abhishek Thakur for hosting these wonderful sessions for people to learn. I look forward to learn more. . Share if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace! .",
            "url": "https://sagarthacker.com/kaggle/2022/01/27/Song-Popularity-EDA.html",
            "relUrl": "/kaggle/2022/01/27/Song-Popularity-EDA.html",
            "date": " • Jan 27, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Text Preprocessing",
            "content": "In any machine learning task, cleaning and pre-processing of the data is a very important step. The better we can represent our data, the better the model training and prediction can be expected. . Specially in the domain of Natural Language Processing (NLP) the data is unstructured. It become crucial to clean and properly format it based on the task at hand. There are various pre-processing steps that can be performed but not necessary to perform all. These steps should be applied based on the problem statement. . Example: Sentiment analysis on twitter data can required to remove hashtags, emoticons, etc. but this may not be the case if we are doing the same analysis on customer feedback data. . Here we are using the twitter_sample dataset from the nltk library. . # Import libraries and load the data import numpy as np import pandas as pd import re import nltk import spacy import string import demoji import contractions import unidecode from num2words import num2words from nltk.corpus import twitter_samples from nltk.corpus import stopwords from nltk.stem.porter import PorterStemmer from nltk.stem.snowball import SnowballStemmer from nltk.stem import WordNetLemmatizer from bs4 import BeautifulSoup from spellchecker import SpellChecker pd.options.display.max_columns=None pd.options.display.max_rows=None pd.options.display.max_colwidth=None nltk.download(&#39;twitter_samples&#39;) # Download the dataset # We are going to use the Negative and Positive Tweets file which each contains 5000 tweets. for name in twitter_samples.fileids(): print(f&#39; - {name}&#39;) . . - negative_tweets.json - positive_tweets.json - tweets.20150430-223406.json . [nltk_data] Downloading package twitter_samples to [nltk_data] C: Users sagar AppData Roaming nltk_data... [nltk_data] Package twitter_samples is already up-to-date! . # Load the negative tweets file and assign label as 0 for negative negative_tweets = twitter_samples.strings(&quot;negative_tweets.json&quot;) df_neg = pd.DataFrame(negative_tweets, columns=[&#39;text&#39;]) df_neg[&#39;label&#39;] = 0 # Load the positive tweets file and assign label as 1 for positive positive_tweets = twitter_samples.strings(&quot;positive_tweets.json&quot;) df_pos = pd.DataFrame(positive_tweets, columns=[&#39;text&#39;]) df_pos[&#39;label&#39;] = 1 df = pd.concat([df_pos, df_neg]) # Concatenate both the files df = df.sample(frac=1).reset_index(drop=True) # Shuffle the data to mix negative and positive tweets . . print(f&#39;Shape of the whole data is: {df.shape[0]} rows and {df.shape[1]} columns&#39;) . Shape of the whole data is: 10000 rows and 2 columns . df.head() . text label . 0 Username Changed! :D | 1 | . 1 @kimtaaeyeonss unnieeee!!!:) | 1 | . 2 @amyewest Thanks! I hope you&#39;ve got a good book to keep you company. :-) | 1 | . 3 :) where are you situated? @Hijay09 | 1 | . 4 @egaroo You&#39;re welcome, I&#39;m glad you liked it :) | 1 | . . Note: Always make it a practice to first skim the dataset before performing any text pre-processing steps. It is important because text data can be very noisy eg. dates are written in different formats, present of accented characters, etc. These are stuff we can easily miss if we don&#8217;t go through the dataset properly. . Lower Casing . Lowercasing is a common text preprocessing technique. It helps to transform all the text in same case. Examples &#39;The&#39;, &#39;the&#39;, &#39;ThE&#39; -&gt; &#39;the&#39; . This is also useful to find all the duplicates since words in different cases are treated as separate words and becomes difficult for us to remove redundant words in all different case combination. . This may not be helpful when we do tasks like Part of Speech tagging (where proper casing gives some information about Nouns and so on) and Sentiment Analysis (where upper casing refers to anger and so on) . df.text = df.text.str.lower() df.head(2) . text label . 0 username changed! :d | 1 | . 1 @kimtaaeyeonss unnieeee!!!:) | 1 | . Remove . URL&#39;s . URL stands for Uniform Resource Locator. If present in a text, it represents the location of another website. . If we are performing any websites backlink analysis, in that case URL&#39;s are useful to keep. Otherwise, they don&#39;t provide any information. So we can remove them from our text. . df.text = df.text.str.replace(r&#39;https?:// S+|www . S+&#39;, &#39;&#39;, regex=True) df.head() . text label . 0 username changed! :d | 1 | . 1 @kimtaaeyeonss unnieeee!!!:) | 1 | . 2 @amyewest thanks! i hope you&#39;ve got a good book to keep you company. :-) | 1 | . 3 :) where are you situated? @hijay09 | 1 | . 4 @egaroo you&#39;re welcome, i&#39;m glad you liked it :) | 1 | . E-mail . E-mail id&#39;s are common in customer feedback data and they do not provide any useful information. So we remove them from the text. . Twitter data that we are using does not contain any email id&#39;s. Hence, please find the code snipper with an dummy example to remove e-mail id&#39;s. . text = &#39;I have being trying to contact xyz via email to xyz@abc.co.in but there is no response.&#39; re.sub(r&#39; S+@ S+&#39;, &#39;&#39;, text) . &#39;I have being trying to contact xyz via email to but there is no response.&#39; . Date . Dates can be represented in various formats and can be difficult at times to remove them. They are unlikely to contain any useful information for predicting the labels. . Below I have used dummy text to showcase the following task. . text = &quot;Today is 22/12/2020 and after two days on 24-12-2020 our vacation starts until 25th.09.2021&quot; # 1. Remove date formats like: dd/mm/yy(yy), dd-mm-yy(yy), dd(st|nd|rd).mm/yy(yy) re.sub(r&#39; d{1,2}(st|nd|rd|th)?[-./] d{1,2}[-./] d{2,4}&#39;, &#39;&#39;, text) . &#39;Today is and after two days on our vacation starts until &#39; . text = &quot;Today is 11th of January, 2021 when I am writing this post. I hope to post this by February 15th or max to max by 20 may 21 or 20th-December-21&quot; # 2. Remove date formats like: 20 apr 21, April 15th, 11th of April, 2021 pattern = re.compile(r&#39;( d{1,2})?(st|nd|rd|th)?[-./,]? s?(of)? s?([J|j]an(uary)?|[F|f]eb(ruary)?|[Mm]ar(ch)?|[Aa]pr(il)?|[Mm]ay|[Jj]un(e)?|[Jj]ul(y)?|[Aa]ug(ust)?|[Ss]ep(tember)?|[Oo]ct(ober)?|[Nn]ov(ember)?|[Dd]ec(ember)?) s?( d{1,2})?(st|nd|rd|th)? s?[-./,]? s?( d{2,4})?&#39;) pattern.sub(r&#39;&#39;, text) . &#39;Today is when I am writing this post. I hope to post this byor max to max by or &#39; . There are various formats in which dates are represented and the above regex can be customized in many ways. Above, &quot;byor&quot; got combined cause we are trying multiple format in single regex pattern. You can customize the above expression accordingly to your need. . HTML Tags . If we are extracting data from various websites, it is possible that the data also contains HTML tags. These tags does not provide any information and should be removed. These tags can be removed using regex or by using BeautifulSoup library. . text = &quot;&quot;&quot; &lt;title&gt;Below is a dummy html code.&lt;/title&gt; &lt;body&gt; &lt;p&gt;All the html opening and closing brackets should be remove.&lt;/p&gt; &lt;a href=&quot;https://www.abc.com&quot;&gt;Company Site&lt;/a&gt; &lt;/body&gt; &quot;&quot;&quot; . pattern = re.compile(&#39;&lt;.*?&gt;&#39;) pattern.sub(&#39;&#39;, text) . &#39; nBelow is a dummy html code. n n All the html opening and closing brackets should be remove. n Company Site n n&#39; . def remove_html(text): clean_text = BeautifulSoup(text).get_text() return clean_text . remove_html(text) . &#39; nBelow is a dummy html code. n nAll the html opening and closing brackets should be remove. nCompany Site n n&#39; . Emojis . As more and more people have started using social media emoji&#39;s play a very crucial role. Emoji&#39;s are used to express emotions that are universally understood. . In some analysis such as sentiment analysis emoji&#39;s can be useful. We can convert them to words or create some new features based on them. For some analysis we need to remove them. Find the below code snippet used to remove the emoji&#39;s. . # Reference: https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b def remove_emoji(text): emoji_pattern = re.compile(&quot;[&quot; u&quot; U0001F600- U0001F64F&quot; # emoticons u&quot; U0001F300- U0001F5FF&quot; # symbols &amp; pictographs u&quot; U0001F680- U0001F6FF&quot; # transport &amp; map symbols u&quot; U0001F1E0- U0001F1FF&quot; # flags (iOS) u&quot; U00002500- U00002BEF&quot; # chinese char u&quot; U00002702- U000027B0&quot; u&quot; U00002702- U000027B0&quot; u&quot; U000024C2- U0001F251&quot; u&quot; U0001f926- U0001f937&quot; u&quot; U00010000- U0010ffff&quot; u&quot; u2640- u2642&quot; u&quot; u2600- u2B55&quot; u&quot; u200d&quot; u&quot; u23cf&quot; u&quot; u23e9&quot; u&quot; u231a&quot; u&quot; ufe0f&quot; # dingbats u&quot; u3030&quot; &quot;]+&quot;, flags=re.UNICODE) return emoji_pattern.sub(r&#39;&#39;, text) . . text = &quot;game is on 🔥🔥. Hilarious😂&quot; remove_emoji(text) . &#39;game is on . Hilarious&#39; . df.text = df.text.apply(lambda x: remove_emoji(x)) . Emoticons . Emoji&#39;s and Emoticons are different. Yes!! Emoticons are used to express facial expressions using keyboard characters such as letters, numbers, and pucntuation marks. Where emjoi&#39;s are small images. . Thanks to Neel Shah for curating a dictionary of emoticons and their description. We shall use this dictionary and remove the emoticons from our text. . EMOTICONS = { u&quot;:‑ )&quot;:&quot;Happy face or smiley&quot;, u&quot;: )&quot;:&quot;Happy face or smiley&quot;, u&quot;:- ]&quot;:&quot;Happy face or smiley&quot;, u&quot;: ]&quot;:&quot;Happy face or smiley&quot;, u&quot;:-3&quot;:&quot;Happy face smiley&quot;, u&quot;:3&quot;:&quot;Happy face smiley&quot;, u&quot;:-&gt;&quot;:&quot;Happy face smiley&quot;, u&quot;:&gt;&quot;:&quot;Happy face smiley&quot;, u&quot;8- )&quot;:&quot;Happy face smiley&quot;, u&quot;:o )&quot;:&quot;Happy face smiley&quot;, u&quot;:- }&quot;:&quot;Happy face smiley&quot;, u&quot;: }&quot;:&quot;Happy face smiley&quot;, u&quot;:- )&quot;:&quot;Happy face smiley&quot;, u&quot;:c )&quot;:&quot;Happy face smiley&quot;, u&quot;: ^ )&quot;:&quot;Happy face smiley&quot;, u&quot;= ]&quot;:&quot;Happy face smiley&quot;, u&quot;= )&quot;:&quot;Happy face smiley&quot;, u&quot;:‑D&quot;:&quot;Laughing, big grin or laugh with glasses&quot;, u&quot;:D&quot;:&quot;Laughing, big grin or laugh with glasses&quot;, u&quot;8‑D&quot;:&quot;Laughing, big grin or laugh with glasses&quot;, u&quot;8D&quot;:&quot;Laughing, big grin or laugh with glasses&quot;, u&quot;X‑D&quot;:&quot;Laughing, big grin or laugh with glasses&quot;, u&quot;XD&quot;:&quot;Laughing, big grin or laugh with glasses&quot;, u&quot;=D&quot;:&quot;Laughing, big grin or laugh with glasses&quot;, u&quot;=3&quot;:&quot;Laughing, big grin or laugh with glasses&quot;, u&quot;B ^D&quot;:&quot;Laughing, big grin or laugh with glasses&quot;, u&quot;:- ) )&quot;:&quot;Very happy&quot;, u&quot;:‑ (&quot;:&quot;Frown, sad, andry or pouting&quot;, u&quot;:- (&quot;:&quot;Frown, sad, andry or pouting&quot;, u&quot;: (&quot;:&quot;Frown, sad, andry or pouting&quot;, u&quot;:‑c&quot;:&quot;Frown, sad, andry or pouting&quot;, u&quot;:c&quot;:&quot;Frown, sad, andry or pouting&quot;, u&quot;:‑&lt;&quot;:&quot;Frown, sad, andry or pouting&quot;, u&quot;:&lt;&quot;:&quot;Frown, sad, andry or pouting&quot;, u&quot;:‑ [&quot;:&quot;Frown, sad, andry or pouting&quot;, u&quot;: [&quot;:&quot;Frown, sad, andry or pouting&quot;, u&quot;:- | |&quot;:&quot;Frown, sad, andry or pouting&quot;, u&quot;&gt;: [&quot;:&quot;Frown, sad, andry or pouting&quot;, u&quot;: {&quot;:&quot;Frown, sad, andry or pouting&quot;, u&quot;:@&quot;:&quot;Frown, sad, andry or pouting&quot;, u&quot;&gt;: (&quot;:&quot;Frown, sad, andry or pouting&quot;, u&quot;:&#39;‑ (&quot;:&quot;Crying&quot;, u&quot;:&#39; (&quot;:&quot;Crying&quot;, u&quot;:&#39;‑ )&quot;:&quot;Tears of happiness&quot;, u&quot;:&#39; )&quot;:&quot;Tears of happiness&quot;, u&quot;D‑&#39;:&quot;:&quot;Horror&quot;, u&quot;D:&lt;&quot;:&quot;Disgust&quot;, u&quot;D:&quot;:&quot;Sadness&quot;, u&quot;D8&quot;:&quot;Great dismay&quot;, u&quot;D;&quot;:&quot;Great dismay&quot;, u&quot;D=&quot;:&quot;Great dismay&quot;, u&quot;DX&quot;:&quot;Great dismay&quot;, u&quot;:‑O&quot;:&quot;Surprise&quot;, u&quot;:O&quot;:&quot;Surprise&quot;, u&quot;:‑o&quot;:&quot;Surprise&quot;, u&quot;:o&quot;:&quot;Surprise&quot;, u&quot;:-0&quot;:&quot;Shock&quot;, u&quot;8‑0&quot;:&quot;Yawn&quot;, u&quot;&gt;:O&quot;:&quot;Yawn&quot;, u&quot;:- *&quot;:&quot;Kiss&quot;, u&quot;: *&quot;:&quot;Kiss&quot;, u&quot;:X&quot;:&quot;Kiss&quot;, u&quot;;‑ )&quot;:&quot;Wink or smirk&quot;, u&quot;; )&quot;:&quot;Wink or smirk&quot;, u&quot; *- )&quot;:&quot;Wink or smirk&quot;, u&quot; * )&quot;:&quot;Wink or smirk&quot;, u&quot;;‑ ]&quot;:&quot;Wink or smirk&quot;, u&quot;; ]&quot;:&quot;Wink or smirk&quot;, u&quot;; ^ )&quot;:&quot;Wink or smirk&quot;, u&quot;:‑,&quot;:&quot;Wink or smirk&quot;, u&quot;;D&quot;:&quot;Wink or smirk&quot;, u&quot;:‑P&quot;:&quot;Tongue sticking out, cheeky, playful or blowing a raspberry&quot;, u&quot;:P&quot;:&quot;Tongue sticking out, cheeky, playful or blowing a raspberry&quot;, u&quot;X‑P&quot;:&quot;Tongue sticking out, cheeky, playful or blowing a raspberry&quot;, u&quot;XP&quot;:&quot;Tongue sticking out, cheeky, playful or blowing a raspberry&quot;, u&quot;:‑Þ&quot;:&quot;Tongue sticking out, cheeky, playful or blowing a raspberry&quot;, u&quot;:Þ&quot;:&quot;Tongue sticking out, cheeky, playful or blowing a raspberry&quot;, u&quot;:b&quot;:&quot;Tongue sticking out, cheeky, playful or blowing a raspberry&quot;, u&quot;d:&quot;:&quot;Tongue sticking out, cheeky, playful or blowing a raspberry&quot;, u&quot;=p&quot;:&quot;Tongue sticking out, cheeky, playful or blowing a raspberry&quot;, u&quot;&gt;:P&quot;:&quot;Tongue sticking out, cheeky, playful or blowing a raspberry&quot;, u&quot;:‑/&quot;:&quot;Skeptical, annoyed, undecided, uneasy or hesitant&quot;, u&quot;:/&quot;:&quot;Skeptical, annoyed, undecided, uneasy or hesitant&quot;, u&quot;:-[.]&quot;:&quot;Skeptical, annoyed, undecided, uneasy or hesitant&quot;, u&quot;&gt;:[( )]&quot;:&quot;Skeptical, annoyed, undecided, uneasy or hesitant&quot;, u&quot;&gt;:/&quot;:&quot;Skeptical, annoyed, undecided, uneasy or hesitant&quot;, u&quot;:[( )]&quot;:&quot;Skeptical, annoyed, undecided, uneasy or hesitant&quot;, u&quot;=/&quot;:&quot;Skeptical, annoyed, undecided, uneasy or hesitant&quot;, u&quot;=[( )]&quot;:&quot;Skeptical, annoyed, undecided, uneasy or hesitant&quot;, u&quot;:L&quot;:&quot;Skeptical, annoyed, undecided, uneasy or hesitant&quot;, u&quot;=L&quot;:&quot;Skeptical, annoyed, undecided, uneasy or hesitant&quot;, u&quot;:S&quot;:&quot;Skeptical, annoyed, undecided, uneasy or hesitant&quot;, u&quot;:‑ |&quot;:&quot;Straight face&quot;, u&quot;: |&quot;:&quot;Straight face&quot;, u&quot;:$&quot;:&quot;Embarrassed or blushing&quot;, u&quot;:‑x&quot;:&quot;Sealed lips or wearing braces or tongue-tied&quot;, u&quot;:x&quot;:&quot;Sealed lips or wearing braces or tongue-tied&quot;, u&quot;:‑#&quot;:&quot;Sealed lips or wearing braces or tongue-tied&quot;, u&quot;:#&quot;:&quot;Sealed lips or wearing braces or tongue-tied&quot;, u&quot;:‑&amp;&quot;:&quot;Sealed lips or wearing braces or tongue-tied&quot;, u&quot;:&amp;&quot;:&quot;Sealed lips or wearing braces or tongue-tied&quot;, u&quot;O:‑ )&quot;:&quot;Angel, saint or innocent&quot;, u&quot;O: )&quot;:&quot;Angel, saint or innocent&quot;, u&quot;0:‑3&quot;:&quot;Angel, saint or innocent&quot;, u&quot;0:3&quot;:&quot;Angel, saint or innocent&quot;, u&quot;0:‑ )&quot;:&quot;Angel, saint or innocent&quot;, u&quot;0: )&quot;:&quot;Angel, saint or innocent&quot;, u&quot;:‑b&quot;:&quot;Tongue sticking out, cheeky, playful or blowing a raspberry&quot;, u&quot;0; ^ )&quot;:&quot;Angel, saint or innocent&quot;, u&quot;&gt;:‑ )&quot;:&quot;Evil or devilish&quot;, u&quot;&gt;: )&quot;:&quot;Evil or devilish&quot;, u&quot; }:‑ )&quot;:&quot;Evil or devilish&quot;, u&quot; }: )&quot;:&quot;Evil or devilish&quot;, u&quot;3:‑ )&quot;:&quot;Evil or devilish&quot;, u&quot;3: )&quot;:&quot;Evil or devilish&quot;, u&quot;&gt;; )&quot;:&quot;Evil or devilish&quot;, u&quot; |;‑ )&quot;:&quot;Cool&quot;, u&quot; |‑O&quot;:&quot;Bored&quot;, u&quot;:‑J&quot;:&quot;Tongue-in-cheek&quot;, u&quot;#‑ )&quot;:&quot;Party all night&quot;, u&quot;%‑ )&quot;:&quot;Drunk or confused&quot;, u&quot;% )&quot;:&quot;Drunk or confused&quot;, u&quot;:-###..&quot;:&quot;Being sick&quot;, u&quot;:###..&quot;:&quot;Being sick&quot;, u&quot;&lt;:‑ |&quot;:&quot;Dump&quot;, u&quot; (&gt;_&lt; )&quot;:&quot;Troubled&quot;, u&quot; (&gt;_&lt; )&gt;&quot;:&quot;Troubled&quot;, u&quot; (&#39;;&#39; )&quot;:&quot;Baby&quot;, u&quot; ( ^ ^&gt;``&quot;:&quot;Nervous or Embarrassed or Troubled or Shy or Sweat drop&quot;, u&quot; ( ^_ ^; )&quot;:&quot;Nervous or Embarrassed or Troubled or Shy or Sweat drop&quot;, u&quot; (-_-; )&quot;:&quot;Nervous or Embarrassed or Troubled or Shy or Sweat drop&quot;, u&quot; (~_~; ) (・ .・; )&quot;:&quot;Nervous or Embarrassed or Troubled or Shy or Sweat drop&quot;, u&quot; (-_- )zzz&quot;:&quot;Sleeping&quot;, u&quot; ( ^_- )&quot;:&quot;Wink&quot;, u&quot; ( ( +_ + ) )&quot;:&quot;Confused&quot;, u&quot; ( +o + )&quot;:&quot;Confused&quot;, u&quot; (o |o )&quot;:&quot;Ultraman&quot;, u&quot; ^_ ^&quot;:&quot;Joyful&quot;, u&quot; ( ^_ ^ )/&quot;:&quot;Joyful&quot;, u&quot; ( ^O ^ )／&quot;:&quot;Joyful&quot;, u&quot; ( ^o ^ )／&quot;:&quot;Joyful&quot;, u&quot; (__ )&quot;:&quot;Kowtow as a sign of respect, or dogeza for apology&quot;, u&quot;_ ( ._ . )_&quot;:&quot;Kowtow as a sign of respect, or dogeza for apology&quot;, u&quot;&lt; (_ _ )&gt;&quot;:&quot;Kowtow as a sign of respect, or dogeza for apology&quot;, u&quot;&lt;m (__ )m&gt;&quot;:&quot;Kowtow as a sign of respect, or dogeza for apology&quot;, u&quot;m (__ )m&quot;:&quot;Kowtow as a sign of respect, or dogeza for apology&quot;, u&quot;m (_ _ )m&quot;:&quot;Kowtow as a sign of respect, or dogeza for apology&quot;, u&quot; (&#39;_&#39; )&quot;:&quot;Sad or Crying&quot;, u&quot; (/_; )&quot;:&quot;Sad or Crying&quot;, u&quot; (T_T ) (;_; )&quot;:&quot;Sad or Crying&quot;, u&quot; (;_;&quot;:&quot;Sad of Crying&quot;, u&quot; (;_: )&quot;:&quot;Sad or Crying&quot;, u&quot; (;O; )&quot;:&quot;Sad or Crying&quot;, u&quot; (:_; )&quot;:&quot;Sad or Crying&quot;, u&quot; (ToT )&quot;:&quot;Sad or Crying&quot;, u&quot;;_;&quot;:&quot;Sad or Crying&quot;, u&quot;;-;&quot;:&quot;Sad or Crying&quot;, u&quot;;n;&quot;:&quot;Sad or Crying&quot;, u&quot;;;&quot;:&quot;Sad or Crying&quot;, u&quot;Q .Q&quot;:&quot;Sad or Crying&quot;, u&quot;T .T&quot;:&quot;Sad or Crying&quot;, u&quot;QQ&quot;:&quot;Sad or Crying&quot;, u&quot;Q_Q&quot;:&quot;Sad or Crying&quot;, u&quot; (- .- )&quot;:&quot;Shame&quot;, u&quot; (-_- )&quot;:&quot;Shame&quot;, u&quot; (一一 )&quot;:&quot;Shame&quot;, u&quot; (；一_一 )&quot;:&quot;Shame&quot;, u&quot; (=_= )&quot;:&quot;Tired&quot;, u&quot; (= ^ · ^= )&quot;:&quot;cat&quot;, u&quot; (= ^ · · ^= )&quot;:&quot;cat&quot;, u&quot;=_ ^= &quot;:&quot;cat&quot;, u&quot; ( . . )&quot;:&quot;Looking down&quot;, u&quot; ( ._ . )&quot;:&quot;Looking down&quot;, u&quot; ^m ^&quot;:&quot;Giggling with hand covering mouth&quot;, u&quot; ( ・ ・?&quot;:&quot;Confusion&quot;, u&quot; (?_? )&quot;:&quot;Confusion&quot;, u&quot;&gt; ^_ ^&lt;&quot;:&quot;Normal Laugh&quot;, u&quot;&lt; ^! ^&gt;&quot;:&quot;Normal Laugh&quot;, u&quot; ^/ ^&quot;:&quot;Normal Laugh&quot;, u&quot; （ * ^_ ^ *）&quot; :&quot;Normal Laugh&quot;, u&quot; ( ^&lt; ^ ) ( ^ . ^ )&quot;:&quot;Normal Laugh&quot;, u&quot; (^ ^ )&quot;:&quot;Normal Laugh&quot;, u&quot; ( ^ . ^ )&quot;:&quot;Normal Laugh&quot;, u&quot; ( ^_ ^ . )&quot;:&quot;Normal Laugh&quot;, u&quot; ( ^_ ^ )&quot;:&quot;Normal Laugh&quot;, u&quot; ( ^ ^ )&quot;:&quot;Normal Laugh&quot;, u&quot; ( ^J ^ )&quot;:&quot;Normal Laugh&quot;, u&quot; ( * ^ . ^ * )&quot;:&quot;Normal Laugh&quot;, u&quot; ( ^— ^ ）&quot;:&quot;Normal Laugh&quot;, u&quot; (# ^ . ^# )&quot;:&quot;Normal Laugh&quot;, u&quot; （ ^— ^ ）&quot;:&quot;Waving&quot;, u&quot; (;_; )/~~~&quot;:&quot;Waving&quot;, u&quot; ( ^ . ^ )/~~~&quot;:&quot;Waving&quot;, u&quot; (-_- )/~~~ ($ · · )/~~~&quot;:&quot;Waving&quot;, u&quot; (T_T )/~~~&quot;:&quot;Waving&quot;, u&quot; (ToT )/~~~&quot;:&quot;Waving&quot;, u&quot; ( * ^0 ^ * )&quot;:&quot;Excited&quot;, u&quot; ( *_ * )&quot;:&quot;Amazed&quot;, u&quot; ( *_ *;&quot;:&quot;Amazed&quot;, u&quot; ( +_ + ) (@_@ )&quot;:&quot;Amazed&quot;, u&quot; ( * ^ ^ )v&quot;:&quot;Laughing,Cheerful&quot;, u&quot; ( ^_ ^ )v&quot;:&quot;Laughing,Cheerful&quot;, u&quot; ( (d[-_-]b ) )&quot;:&quot;Headphones,Listening to music&quot;, u&#39; (-&quot;- )&#39;:&quot;Worried&quot;, u&quot; (ーー; )&quot;:&quot;Worried&quot;, u&quot; ( ^0_0 ^ )&quot;:&quot;Eyeglasses&quot;, u&quot; ( ＾ｖ ＾ )&quot;:&quot;Happy&quot;, u&quot; ( ＾ｕ ＾ )&quot;:&quot;Happy&quot;, u&quot; ( ^ )o ( ^ )&quot;:&quot;Happy&quot;, u&quot; ( ^O ^ )&quot;:&quot;Happy&quot;, u&quot; ( ^o ^ )&quot;:&quot;Happy&quot;, u&quot; ) ^o ^ (&quot;:&quot;Happy&quot;, u&quot;:O o_O&quot;:&quot;Surprised&quot;, u&quot;o_0&quot;:&quot;Surprised&quot;, u&quot;o .O&quot;:&quot;Surpised&quot;, u&quot; (o .o )&quot;:&quot;Surprised&quot;, u&quot;oO&quot;:&quot;Surprised&quot;, u&quot; ( *￣m￣ )&quot;:&quot;Dissatisfied&quot;, u&quot; (‘A` )&quot;:&quot;Snubbed or Deflated&quot; } . . def remove_emoticons(text): emoticons_pattern = re.compile(u&#39;(&#39; + u&#39;|&#39;.join(emo for emo in EMOTICONS) + u&#39;)&#39;) return emoticons_pattern.sub(r&#39;&#39;, text) . remove_emoticons(&quot;Hello :-&gt;&quot;) . &#39;Hello &#39; . df.text = df.text.apply(lambda x: remove_emoticons(x)) . Hashtags and Mentions . We are habituated to use hashtags and mentions in our tweet either to indicate the context or bring attention to an individual. Hashtags can be used to extract features, to see what&#39;s trending and in various other applications. . Since, we don&#39;t require them we&#39;ll remove them. . def remove_tags_mentions(text): pattern = re.compile(r&#39;(@ S+|# S+)&#39;) return pattern.sub(&#39;&#39;, text) . text = &quot;live @flippinginja on #younow - jonah and jareddddd&quot; remove_tags_mentions(text) . &#39;live on - jonah and jareddddd&#39; . df.text = df.text.apply(lambda x: remove_tags_mentions(x)) . Punctuations . Punctuations are character other than alphaters and digits. These include [!&quot;#$%&amp; &#39;()*+,-./:;&lt;=&gt;?@ ^_`{|}~] . It is better remove or convert emoticons before removing the punctuations, since if we do the other we around we might loose the emoticons from the text. Another example, if the text contains $10.50 then we&#39;ll remove the .(dot) and the value will loose it&#39;s meaning. . PUNCTUATIONS = string.punctuation def remove_punctuation(text): return text.translate(str.maketrans(&#39;&#39;, &#39;&#39;, PUNCTUATIONS)) . df.text = df[&quot;text&quot;].apply(lambda text: remove_punctuation(text)) . text label . 0 username changed d | 1 | . 1 unnieeee | 1 | . 2 thanks i hope youve got a good book to keep you company | 1 | . 3 where are you situated | 1 | . 4 youre welcome im glad you liked it | 1 | . Stopwords . Stopwords are commonly occuring words in any language. Such as, in english these words are &#39;the&#39;, &#39;a&#39;, &#39;an&#39;, &amp; many more. They are in most cases not useful and should be removed. . There are certain tasks in which these words are useful such as Part-of-Speech(POS) tagging, language translation. Stopwords are compiled for many languages, for english language we can use the list from the nltk package. . STOPWORDS = set(stopwords.words(&#39;english&#39;)) def remove_stopwords(text): return &#39; &#39;.join([word for word in text.split() if word not in STOPWORDS]) . df.text = df.text.apply(lambda text: remove_stopwords(text)) df.head() . text label . 0 username changed | 1 | . 1 unnieeee | 1 | . 2 thanks hope youve got good book keep company | 1 | . 3 situated | 1 | . 4 youre welcome im glad liked | 1 | . Numbers . We may remove numbers if they are not useful in our analysis. But analysis in the financial domain, numbers are very useful. . df.text = df.text.str.replace(r&#39; d+&#39;, &#39;&#39;, regex=True) . Extra whitespaces . After usually after preprocessing the text there might be extra whitespaces that might be created after transforming, removing various characters. Also, there is a need to remove all the new line, tab characters as well from our text. . def remove_whitespaces(text): return &quot; &quot;.join(text.split()) . text = &quot; Whitespaces in the beginning are removed t as well n as in between the text &quot; clean_text = &quot; &quot;.join(text.split()) clean_text . &#39;Whitespaces in the beginning are removed as well as in between the text&#39; . df.text = df.text.apply(lambda x: remove_whitespaces(x)) . Frequent words . Previously we have removed stopwords which are common in any language. If we are working in any domain, we can also remove the common words used in that domain which don&#39;t provide us with much information. . def freq_words(text): tokens = word_tokenize(text) FrequentWords = [] for word in tokens: counter[word] += 1 for (word, word_count) in counter.most_common(10): FrequentWords.append(word) return FrequentWords def remove_fw(text, FrequentWords): tokens = word_tokenize(text) without_fw = [] for word in tokens: if word not in FrequentWords: without_fw.append(word) without_fw = &#39; &#39;.join(without_fw) return without_fw counter = Counter() . text = &quot;&quot;&quot; Natural Language Processing is the technology used to aid computers to understand the human’s natural language. It’s not an easy task teaching machines to understand how we communicate. Leand Romaf, an experienced software engineer who is passionate at teaching people how artificial intelligence systems work, says that “in recent years, there have been significant breakthroughs in empowering computers to understand language just as we do.” This article will give a simple introduction to Natural Language Processing and how it can be achieved. Natural Language Processing, usually shortened as NLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable. Most NLP techniques rely on machine learning to derive meaning from human languages. &quot;&quot;&quot; . FrequentWords = freq_words(text) print(FrequentWords) . [&#39;,&#39;, &#39;to&#39;, &#39;.&#39;, &#39;is&#39;, &#39;the&#39;, &#39;understand&#39;, &#39;Natural&#39;, &#39;Language&#39;, &#39;Processing&#39;, &#39;computers&#39;] . fw_result = remove_fw(text, FrequentWords) fw_result . &#39;technology used aid human ’ s natural language It ’ s not an easy task teaching machines how we communicate Leand Romaf an experienced software engineer who passionate at teaching people how artificial intelligence systems work says that “ in recent years there have been significant breakthroughs in empowering language just as we do. ” This article will give a simple introduction and how it can be achieved usually shortened as NLP a branch of artificial intelligence that deals with interaction between and humans using natural language The ultimate objective of NLP read decipher and make sense of human languages in a manner that valuable Most NLP techniques rely on machine learning derive meaning from human languages&#39; . Rare words . Rare words are similar to frequent words. We can remove them because they are so less that they cannot add any value to the purpose. . def rare_words(text): # tokenization tokens = word_tokenize(text) for word in tokens: counter[word]= +1 RareWords = [] number_rare_words = 10 # take top 10 frequent words frequentWords = counter.most_common() for (word, word_count) in frequentWords[:-number_rare_words:-1]: RareWords.append(word) return RareWords def remove_rw(text, RareWords): tokens = word_tokenize(text) without_rw = [] for word in tokens: if word not in RareWords: without_rw.append(word) without_rw = &#39; &#39;.join(without_fw) return without_rw counter = Counter() . text = &quot;&quot;&quot; Natural Language Processing is the technology used to aid computers to understand the human’s natural language. It’s not an easy task teaching machines to understand how we communicate. Leand Romaf, an experienced software engineer who is passionate at teaching people how artificial intelligence systems work, says that “in recent years, there have been significant breakthroughs in empowering computers to understand language just as we do.” This article will give a simple introduction to Natural Language Processing and how it can be achieved. Natural Language Processing, usually shortened as NLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable. Most NLP techniques rely on machine learning to derive meaning from human languages. &quot;&quot;&quot; . RareWords = rare_words(text) RareWords . [&#39;from&#39;, &#39;meaning&#39;, &#39;derive&#39;, &#39;learning&#39;, &#39;machine&#39;, &#39;on&#39;, &#39;rely&#39;, &#39;techniques&#39;, &#39;Most&#39;] . rw_result = remove_fw(text, RareWords) rw_result . &#39;Natural Language Processing is the technology used to aid computers to understand the human ’ s natural language . It ’ s not an easy task teaching machines to understand how we communicate . Leand Romaf , an experienced software engineer who is passionate at teaching people how artificial intelligence systems work , says that “ in recent years , there have been significant breakthroughs in empowering computers to understand language just as we do. ” This article will give a simple introduction to Natural Language Processing and how it can be achieved . Natural Language Processing , usually shortened as NLP , is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language . The ultimate objective of NLP is to read , decipher , understand , and make sense of the human languages in a manner that is valuable . NLP to human languages .&#39; . Conversion of Emoji to Words . To remove or not is done based on the purpose of the application. Example if we are building a sentiment analysis system emoji&#39;s can be useful. . &quot;The movie was 🔥&quot; or &quot;The movie was 💩&quot; . If we remove the emoji&#39;s the meaning of the sentence changes completely. In these cases we can convert emoji&#39;s to words. . demoji requires an initial data download from the Unicode Consortium&#39;s emoji code repository. . On first use of the package, call download_codes(). This will store the Unicode hex-notated symbols at ~/.demoji/codes.json for future use. . Read more about demoji on pypi.org . demoji.download_codes() . Downloading emoji data ... ... OK (Got response in 1.35 seconds) Writing emoji data to C: Users sagar .demoji codes.json ... ... OK . def emoji_to_words(text): return demoji.replace_with_desc(text, sep=&quot;__&quot;) . text = &quot;game is on 🔥 🚣🏼&quot; emoji_to_words(text) . &#39;game is on __fire__ __person rowing boat: medium-light skin tone__&#39; . Conversion of Emoticons to Words . As we did for emoji&#39;s, we convert emoticons to words for the same purpose. . def emoticons_to_words(text): for emot in EMOTICONS: text = re.sub(u&#39;(&#39;+emot+&#39;)&#39;, &quot;_&quot;.join(EMOTICONS[emot].replace(&quot;,&quot;,&quot;&quot;).replace(&quot;:&quot;,&quot;&quot;).split()), text) return text . text = &quot;Hey there!! :-)&quot; emoticons_to_words(text) . &#39;Hey there!! Happy_face_smiley&#39; . Converting Numbers to Words . If our analysis require us to use information based on the numbers in the text, we can convert them to words. . Read more about num2words on github . def nums_to_words(text): new_text = [] for word in text.split(): if word.isdigit(): new_text.append(num2words(word)) else: new_text.append(word) return &quot; &quot;.join(new_text) . text = &quot;I ran this track 30 times&quot; nums_to_words(text) . &#39;I ran this track thirty times&#39; . Chat words Conversion . The more we use social media, we have become lazy to type the whole phrase or word. Due to which slang words came into existance such as &quot;omg&quot; which represents &quot;Oh my god&quot;. Such slang words don&#39;t provide much information and if we need to use them we have to convert them. . Thank you: GitHub repo for the list of slang words . chat_words = &quot;&quot;&quot; AFAIK=As Far As I Know AFK=Away From Keyboard ASAP=As Soon As Possible ATK=At The Keyboard ATM=At The Moment A3=Anytime, Anywhere, Anyplace BAK=Back At Keyboard BBL=Be Back Later BBS=Be Back Soon BFN=Bye For Now B4N=Bye For Now BRB=Be Right Back BRT=Be Right There BTW=By The Way B4=Before B4N=Bye For Now CU=See You CUL8R=See You Later CYA=See You FAQ=Frequently Asked Questions FC=Fingers Crossed FWIW=For What It&#39;s Worth FYI=For Your Information GAL=Get A Life GG=Good Game GN=Good Night GMTA=Great Minds Think Alike GR8=Great! G9=Genius IC=I See ICQ=I Seek you (also a chat program) ILU=ILU: I Love You IMHO=In My Honest/Humble Opinion IMO=In My Opinion IOW=In Other Words IRL=In Real Life KISS=Keep It Simple, Stupid LDR=Long Distance Relationship LMAO=Laugh My A.. Off LOL=Laughing Out Loud LTNS=Long Time No See L8R=Later MTE=My Thoughts Exactly M8=Mate NRN=No Reply Necessary OIC=Oh I See PITA=Pain In The A.. PRT=Party PRW=Parents Are Watching QPSA?=Que Pasa? ROFL=Rolling On The Floor Laughing ROFLOL=Rolling On The Floor Laughing Out Loud ROTFLMAO=Rolling On The Floor Laughing My A.. Off SK8=Skate STATS=Your sex and age ASL=Age, Sex, Location THX=Thank You TTFN=Ta-Ta For Now! TTYL=Talk To You Later U=You U2=You Too U4E=Yours For Ever WB=Welcome Back WTF=What The F... WTG=Way To Go! WUF=Where Are You From? W8=Wait... 7K=Sick:-D Laugher OMG=Oh my god&quot;&quot;&quot; . . chat_words_dict = dict() chat_words_set = set() def cw_conversion(text): new_text = [] for word in text.split(): if word.upper() in chat_words_set: new_text.append(chat_words_dict[word.upper()]) else: new_text.append(word) return &quot; &quot;.join(new_text) for line in chat_words.split(&#39; n&#39;): if line != &#39;&#39;: cw, cw_expanded = line.split(&#39;=&#39;)[0], line.split(&#39;=&#39;)[1] chat_words_set.add(cw) chat_words_dict[cw] = cw_expanded . text = &quot;omg that&#39;s awesome.&quot; cw_conversion(text) . &#34;Oh my god that&#39;s awesome.&#34; . Expanding Contractions . Contractions are words or combinations of words created by dropping a few letters and replacing those letters by an apostrophe. . Example: . don&#39;t: do not | we&#39;ll: we will | . Our nlp model don&#39;t understand these contractions i.e. they don&#39;t understand that &quot;don&#39;t&quot; and &quot;do not&quot; are the same thing. If our problem statement requires them then we can expand them or else leave it as it is. . def expand_contractions(text): expanded_text = [] for line in text: expanded_text.append(contractions.fix(line)) return expanded_text . text = [&quot;I&#39;ll be there within 15 minutes.&quot;, &quot;It&#39;s awesome to meet your new friends.&quot;] expand_contractions(text) . [&#39;I will be there within 15 minutes.&#39;, &#39;it is awesome to meet your new friends.&#39;] . Stemming . In stemming we reduce the word to it&#39;s base or root form by removing the suffix characters from the word. It is one of the technique to normalize text. . Stemming for root word &quot;like&quot; include: . &quot;likes&quot; | &quot;liked&quot; | &quot;likely&quot; | &quot;liking&quot; | . Stemmed word doesn&#39;t always match the words in our dictionary such as: . console -&gt; consol | company -&gt; compani | welcome -&gt; welcom | . Due to which stemming is not performed in all nlp tasks. . There are various algorithms used for stemming but the most widely used is PorterStemmer. In this post we have used the PorterStemmer as well. . stemmer = PorterStemmer() def stem_words(text): return &#39; &#39;.join([stemmer.stem(word) for word in text.split()]) . df[&#39;text_stemmed&#39;] = df.text.apply(lambda text: stem_words(text)) df[[&#39;text&#39;, &#39;text_stemmed&#39;]].head() . text text_stemmed . 0 username changed | usernam chang | . 1 unnieeee | unnieee | . 2 thanks hope youve got good book keep company | thank hope youv got good book keep compani | . 3 situated | situat | . 4 youre welcome im glad liked | your welcom im glad like | . PorterStemmer can be used only for english. If we are working with other than english then we can use SnowballStemmer. . SnowballStemmer.languages . (&#39;arabic&#39;, &#39;danish&#39;, &#39;dutch&#39;, &#39;english&#39;, &#39;finnish&#39;, &#39;french&#39;, &#39;german&#39;, &#39;hungarian&#39;, &#39;italian&#39;, &#39;norwegian&#39;, &#39;porter&#39;, &#39;portuguese&#39;, &#39;romanian&#39;, &#39;russian&#39;, &#39;spanish&#39;, &#39;swedish&#39;) . Lemmatization . Lemmatization tried to perform the similar task as that of stemming i.e. trying to reduce the inflection words to it&#39;s base form. But lemmatization does it by using a different approach. . Lemmatizations takes into consideration of the morphological analysis of the word. It tries to reduce to words to it&#39;s dictionary form which is known as lemma. . lemmatizer = WordNetLemmatizer() def text_lemmatize(text): return &#39; &#39;.join([lemmatizer.lemmatize(word) for word in text.split()]) . df[&#39;text_lemmatized&#39;] = df.text.apply(lambda text: text_lemmatize(text)) df[[&#39;text&#39;, &#39;text_stemmed&#39;, &#39;text_lemmatized&#39;]].head() . text text_stemmed text_lemmatized . 0 username changed | usernam chang | username changed | . 1 unnieeee | unnieee | unnieeee | . 2 thanks hope youve got good book keep company | thank hope youv got good book keep compani | thanks hope youve got good book keep company | . 3 situated | situat | situated | . 4 youre welcome im glad liked | your welcom im glad like | youre welcome im glad liked | . Difference between Stemming and Lemmatization: . Stemming Lemmatization . Fast compared to lemmatization | Slow compared to stemming | . Reduces the word to it&#39;s base form by removing the suffix | Uses lexical knowledge to get the base form of the word | . Does not always provide meaning or dictionary form of the original word | Resulting words are always meaningful and dictionary words | . Spelling Correction . We as human always make mistake. Normally incorrect spelling in text are know as typos. . Since the NLP model doesn&#39;t know the difference between a correct and an incorrect word. For the model &quot;thanks&quot; and &quot;thnks&quot; are two different words. Therefore, spelling correction is an important step to bring the incorrect words in the correct format. . spell = SpellChecker() def correct_spelling(text): correct_text = [] misspelled_words = spell.unknown(text.split()) for word in text.split(): if word in misspelled_words: correct_text.append(spell.correction(word)) else: correct_text.append(word) return &quot; &quot;.join(correct_text) . text = &quot;Hi, hwo are you doin? I&#39;m good thnks for asking&quot; correct_spelling(text) . &#34;Hi, how are you doing I&#39;m good thanks for asking&#34; . text = &quot;hw are you doin? I&#39;m god thnks&quot; correct_spelling(text) . &#34;he are you doing I&#39;m god thanks&#34; . Convert accented characters to ASCII characters . Accent marks (also referred to as diacritics or diacriticals) usually appear above a character when we press the character for a long time. These need to be remove cause the model cannot distinguish between &quot;dèèp&quot; and &quot;deep&quot;. It will consider them as two different words. . def accented_to_ascii(text): return unidecode.unidecode(text) . text = &quot;This is an example text with accented characters like dèèp lèarning ánd cömputer vísíön etc.&quot; accented_to_ascii(text) . &#39;This is an example text with accented characters like deep learning and computer vision etc.&#39; . Conclusion . In this article, most of the text pre-processing techniques are explanied. I&#39;ll update this post as I learn more techniques to pre-process text. . Share if you liked it, comment if you loved it. Hope to see you guys in the next one. Peace! .",
            "url": "https://sagarthacker.com/text%20preprocessing/natural%20language%20processing/2021/04/11/Text-Preprocessing.html",
            "relUrl": "/text%20preprocessing/natural%20language%20processing/2021/04/11/Text-Preprocessing.html",
            "date": " • Apr 11, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "What is Natural Language Processing?",
            "content": "Natural language processing is a branch of Artificial Intelligence which aims to bridge the gap between how a computer and human communicate with each other. The two major handles used for communication are speech and written i.e. text. . History of Natural Language Processing . The dawn of NLP can be dated back to the early 1900s. In 1950, Alan Turing published his famous article “Computing Machinery and Intelligence” which proposed what is now called the Turing test as a criterion of intelligence. It tests the ability of the computer program to impersonate a human in a real-time conversation with a human judge where the judge is unable to distinguish the human from the computer program. In 1954, the Georgetown experiment automatically translated more than sixty Russian words into English. . In 1957, Noam Chomsky’s Syntactic Structures a rule-based system of syntactic structures with “universal grammar” was an incredible advancement. Up to the 1980’s most of the NLP systems were based on complex hand-written rules but in the late 1980s by the introduction of machine learning algorithms for language processing revolutionized the field. A steady increase in computational power resulting from Moore’s law and use of statistical models that use probabilistic measures to map the features making up the input data. Watson an artificial intelligence software designed as a question answering system won the Jeopardy contest, defeating the best human players in February 2011. . Development of famous virtual assistants like Siri in 2011, Amazon Alexa in 2014, and Google Assistant in 2016. The use of deep learning produced better results than the state-of-the-art in many natural language processing tasks, such as machine translation, text classification, and many more. Recent advancements include the use of network architecture of the transformer which is based on the attention mechanism that has produced better results in various NLP tasks. . We humans in our daily life overlook the powerful ability of our human brain to read, understand the meaning of a word, it’s context (how does it relate to each other), understand humor, sarcasm, and thousand other things. How do we teach this to a computer? . Challenges . 1. Ambiguity: In a natural language, words are unique but their meaning may differ based on the context in which it is used. One classical example used is: . The bank is a financial institution where customers can save or borrow money. | Tom was sitting by the banks of the river. | . In this example, we can see that the word “bank” is used in two different ways. The word is the same but the meaning is different. This is because the context in which the word is used is different. . 2. Co-Referencing: It is a process to find all the phrases in the document that refer to the same entity. Example: Harry kept the paneer on the plate and ate it. Here it refers to the paneer that he ate which was kept on the plate. . 3. Information Extraction: Identifying phrases in a language that refer to specific types of entities and relations in text. Named Entity Recognition (NER) is the task used to identify the names of people, organizations, places, etc, in a text. Example: Tom used to work at FedEx and lives in Mumbai, India. where Person = Tom, organization = FedEx and Place = Mumbai, India . 4. Personality, intention, emotions, and style: Different authors may have different personalities, intentions, emotions, and styles to convey the same idea. Based on these factors the underlying idea can be interpreted in different ways. Use of humor or sarcasm may convey a meaning that is opposite of the literal one. . Applications . 1. Machine Translation: The idea behind machine translation is to develop a system that is capable of translating text from one language to another without any human intervention. Only translating the text from one language to another is not the key. Understanding the meaning behind the text and translating it to some other language is the crux of it. Example: Google Translate . 2. Automatic summarization: We all love to read storybooks and always a good storybook will have a summary at the end that highlights the important things about the story. Likewise take any piece of text, a story, a news article, etc, and develop a system that can automatically summary the piece of text. Example: Inshorts – an app that summarizes each news article in 60 words. . 3. Sentiment Analysis: It deals with the study of extracting opinions and sentiment that are not always explicitly expressed. For instance it helps the company to understand the level of consumer satisfaction for its goods and services. Example: “I love the new iPhone and it has a great camera.”. . Another branch of sentiment analysis is “Aspect based Sentiment Analysis” where it tries to extract opinions for each aspect from the customer review. Example: “The new iPhone is great it has a great camera but the battery life is not that good.” Here the customer is satisfied with the camera aspect of the iPhone but not the battery life. . 4. Text Classification: Organizing text documents into predefined categories enables to classify the information or any activity. Examples: Classifying an email as spam or not spam. . 5. Question Answering: Question answering deals with a system that can answer questions posed by humans in natural language. Sounds simple yet building the knowledge base, understanding the text, and to answer in natural language is altogether a thing in itself. . 6. Chatbots: Chatbots are a piece of software that can simulate a conversation (or chat) with a user in natural language through websites, apps, messaging applications, etc. Chatbots are a natural evolution of question answering system but are one step further with their ability to understand the text and engage in a conversation. . 7. Speech Recognition: Using our voice to interact with our phones has become a common phenomenon. For example to ask questions to our voice assistants like Google Assistant/Siri/Cortana, use of voice to type a piece of text. Recognizing speech has replaced the method by which we interact with our devices and made it so convenient. . Recent advancements in NLP have deepened our knowledge on how to tackle the various challenges in NLP. Also, this new decade will be filled with excitement and breakthroughs that awaits us. Stay tunned to deep dive into the world of NLP. . Share if you like it, comment if you loved it. Hope to see you guys in the next one. Peace! .",
            "url": "https://sagarthacker.com/natural%20language%20processing/2021/04/05/What-is-NLP.html",
            "relUrl": "/natural%20language%20processing/2021/04/05/What-is-NLP.html",
            "date": " • Apr 5, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Path to become a Machine Learning Expert",
            "content": "Path to becoming a Machine Learning (ML) Expert made easy. There are a lot of resources out there that can be overwhelming at the start. But don’t worry this learning path would provide structure and lay the foundational knowledge to begin a career in ML. . 1. Learn the basics of Descriptive Statistics, Inferential Statistics and Math used in Machine Learning . Understanding the math used in ML can help in building the foundation strong. Udacity offers courses on descriptive statistics and inferential statistics. These courses are free and use excel to teach the concepts. . Along with statistics and probabilities, concepts on linear algebra, multivariate calculus, optimization functions and many more form the building blocks for ML. There is an awesome youtube channel that makes these concepts very easy to learn. 3Brown1Blue focuses on teaching mathematics using a distinct visual perspective. . More resources: . Computational Linear Algebra for Coders | Prof. Gilbert Strang’s Linear Algebra book/course | Matrix Cookbook by Kaare Brandt Petersen &amp; Michael Syskind Pedersen | Think Stats (Exploratory Data Analysis in Python) by Allen Downey | Convex Optimization by Stephen Boyd and Lieven Vandenberghe | Essentials of Metaheuristics by Sean Luke | . 2. Learn the basics of Python and it’s packages . First, let’s install Python. The easiest way to do this is by installing Anaconda. All the packages that are required come along with Anaconda. . You can start from learning the basics of Python i.e. data structures, functions, class, etc. and it’s libraries. I started learning about python in my college days, I read the book Learn Python the Hard Way. A very good book for beginners. Introduction to Python Programming by Udacity is a free course that covers the basics of Python. Introduction to Python is another free course by Analytics Vidhya. Another free course by Google is Google’s Python Class. . Next, learn about how to use Regular Expression (also called regex) in Python. It will come in use for data cleaning, especially if you are working with text data. Learn regular expressions through Google class. A very good beginner tutorial for learning regular expression in python on Analytics Vidhya. Cheatsheet for Regex. . Now comes the fun part of learning the various libraries in Python. Numpy, Pandas, Matplotlib, Seaborn, and Sklearn are the packages heavily used in ML. . Numpy provides a high-performance multidimensional array and basic tools to compute with and manipulate these arrays. Numpy quickstart tutorial is a good place to start. This will form a good foundation for this to come. Practice numpy by solving 100 numpy exercises to solve. | Pandas is used for data manipulation and analysis. The most used package in Python is Pandas. Intro to pandas data structure provides a detailed tutorial on pandas. A short course by Kaggle on pandas. | Matplotlib is a visualization library in python. In the matplotlib tutorial, you will learn the basics of Python data visualization, the anatomy of a Matplotlib plot, and much more. Official documentation of matplotlib is one of the best ways to learn the library. | Seaborn is another visualization library built on top of matplotlib. Kaggle short course on data visualization provides a good start point to learn the library. | . 3. Data Exploration/Cleaning/Preparation . Real-world data is unstructured, contains missing values, outliers, typos, etc. This step is one of the most important steps for a data analyst to perform because how good the model will perform will depend on the quality of the data. . Learn different stages of data explorations: . Variable Identification, Univariate and Multivariate analysis | Missing values treatment | Outlier treatment | Feature Engineering | Additional resources: . You can also refer to the data exploration guide. | Book on Python for Data Analysis by Wes McKinney | . 4. Introduction to Machine Learning . Now it’s time to enter the belly of the beast. There are various resources to learn ML and I would suggest the following courses: . Machine Learning by Stanford (Coursera) The Machine Learning course by Andrew Ng is one of the best courses out there and covers all the basic algorithms. Also, it introduces all the advanced topics in a very simple manner which is easy to understand. However, this course is taught in Octave rather than the popular languages like R/Python. Also, this course is NOT free but you can apply for financial aid. . | Machine Learning A-Z™: Hands-On Python &amp; R In Data Science (Udemy) Good course for beginners. Explore complex topics such as natural language processing (NLP), reinforcement learning (RL), deep learning (DL) among many others. Tons of practice exercise and quizzes. This course is NOT free but comparatively not expensive. . | Machine Learning (edx) This is an advanced course that has the highest math prerequisite out of any other course in this list. You’ll need a very firm grasp of Linear Algebra, Calculus, Probability, and programming. This course is free of cost but to acquire a certificate payment is required. . | Comprehensive learning path for Data Science (Analytics Vidhya) This course covers every topic right from the beginning. Installing Python, data cleaning and preparation, Machine learning concepts, deep learning, and NLP. This course is free and does not come with any certification. . | Books: . The Hundred Page Machine Learning Book | The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition | List of best books for machine learning. . After learning about the various techniques in ML the next natural thing to do is apply those techniques. What better place than Kaggle. It is one of the most popular websites among data science enthusiasts. Below two problem statement can be a good starting problem statement to begin with. . Titanic: Machine Learning from Disaster | House Prices: Advanced Regression Techniques | 5. Deep Learning . Using the idea to mimic a human brain has been around since the 1900s. There were various algorithms and techniques developed for the same but due to the lack of computing power, it was difficult to run those algorithms. . Due to the improvements in the hardware and the introduction to using GPUs to compute caught the attention of people passionate about working on neural net-based models. Today, state of the art results can be obtained using deep neural networks. . Courses from deeplearning.ai on Coursera are one of the most popular and fantastic courses on deep learning. . Neural Networks and Deep Learning | Deep Learning Specialization | Both the courses are paid but financial aid is available for both of them. . Additional Resources: . Deep Learning Summer School, Montreal 2015 | Deep Learning for Perception, Virginia Tech, Electrical, and Computer Engineering | CS231N 2017 | A blog that explains concepts on Convolutional Neural Nets (CNN) | (Book) Deep Learning – Methods and Applications | (Youtube Channel) DeepLearning.TV | Deep Learning book from MIT | Neural Networks and Deep Learning online Book | Comprehensive resources on deeplearning.net | 6. Natural Language Processing . Natural language processing (NLP) is a branch of Artificial Intelligence which aims to bridge the gap between how a computer and human communicate with each other. The two major handles used for communication are speech and written i.e. text. If you are unfamiliar with what NLP is, this blog could help in understanding what NLP is. . Courses: . (Youtube) Natural Language Processing by University of Michigan | Speech and Language Processing | Stanford CS224N: NLP with Deep Learning Winter 2019 – Stanford | Lecture Collection on Natural Language Processing with Deep Learning (Winter 2017) – Stanford | CS224d: Deep Learning for Natural Language Processing – Stanford | Natural Language Processing Specialization offered by deeplearning.ai on Coursera (Intermediate level) | Natural Language Processing offered by National Research University Higher School of Economics on Coursera (Advanced level course) | Machine Learning in itself is a huge domain and the only way to master it is to explore and practice. I cannot stress more on practice because without practice is like trying to play the guitar without any strings. . Popular blogs to follow: . Analytics Vidhya | Machine Learning Mastery | Towards Data Science | KDnuggets | Additional Resources: . A Complete Python Tutorial to Learn Data Science from Scratch | A Comprehensive Learning Path for Deep Learning in 2019 on Analytics Vidhya | Learning Path to Master Computer Vision in 2020 on Analytics Vidhya | A Comprehensive Learning Path to Understand and Master NLP in 2020 on Analytics Vidhya | A Comprehensive Guide to Understand and Implement Text Classification in Python on Analytics Vidhya | Collection of datasets for NLP | A comprehensive Learning path to becoming a data scientist in 2020 free course on Analytics Vidhya | I wish you all the best on your journey to becoming a machine learning expert. . Share if you like it, comment if you loved it. Hope to see you guys in the next one. Peace! .",
            "url": "https://sagarthacker.com/machine%20learning/learning%20path/2021/04/05/Path-to-become-a-Machine-Learning-Expert.html",
            "relUrl": "/machine%20learning/learning%20path/2021/04/05/Path-to-become-a-Machine-Learning-Expert.html",
            "date": " • Apr 5, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "A Data Scientist who is passionate about solving pressing challenges across computational linguistics, healthcare, and information access. Proficient in developing complex machine learning and statistical modeling algorithms/techniques for identifying patterns and extracting valuable insights. . Links . Portfolio | LinkedIn | GitHub | Twitter | .",
          "url": "https://sagarthacker.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sagarthacker.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}